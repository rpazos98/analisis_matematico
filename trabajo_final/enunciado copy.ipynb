{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpJ7s_SIVu_I"
      },
      "source": [
        "# Trabajo Práctico Final: Linear/Quadratic Discriminant Analysis (LDA/QDA)\n",
        "\n",
        "### Definición: Clasificador Bayesiano\n",
        "\n",
        "Sean $k$ poblaciones, $x \\in \\mathbb{R}^p$ puede pertenecer a cualquiera $g \\in \\mathcal{G}$ de ellas. Bajo un esquema bayesiano, se define entonces $\\pi_j \\doteq P(G = j)$ la probabilidad *a priori* de que $X$ pertenezca a la clase *j*, y se **asume conocida** la distribución condicional de cada observable dado su clase $f_j \\doteq f_{X|G=j}$.\n",
        "\n",
        "De esta manera dicha probabilidad *a posteriori* resulta\n",
        "$$\n",
        "P(G|_{X=x} = j) = \\frac{f_{X|G=j}(x) \\cdot p_G(j)}{f_X(x)} \\propto f_j(x) \\cdot \\pi_j\n",
        "$$\n",
        "\n",
        "La regla de decisión de Bayes es entonces\n",
        "$$\n",
        "H(x) \\doteq \\arg \\max_{g \\in \\mathcal{G}} \\{ P(G|_{X=x} = j) \\} = \\arg \\max_{g \\in \\mathcal{G}} \\{ f_j(x) \\cdot \\pi_j \\}\n",
        "$$\n",
        "\n",
        "es decir, se predice a $x$ como perteneciente a la población $j$ cuya probabilidad a posteriori es máxima.\n",
        "\n",
        "*Ojo, a no desesperar! $\\pi_j$ no es otra cosa que una constante prefijada, y $f_j$ es, en su esencia, un campo escalar de $x$ a simplemente evaluar.*\n",
        "\n",
        "### Distribución condicional\n",
        "\n",
        "Para los clasificadores de discriminante cuadrático y lineal (QDA/LDA) se asume que $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma_j)$, es decir, se asume que cada población sigue una distribución normal.\n",
        "\n",
        "Por definición, se tiene entonces que para una clase $j$:\n",
        "$$\n",
        "f_j(x) = \\frac{1}{(2 \\pi)^\\frac{p}{2} \\cdot |\\Sigma_j|^\\frac{1}{2}} e^{- \\frac{1}{2}(x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)}\n",
        "$$\n",
        "\n",
        "Aplicando logaritmo (que al ser una función estrictamente creciente no afecta el cálculo de máximos/mínimos), queda algo mucho más práctico de trabajar:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "Observar que en este caso $C=-\\frac{p}{2} \\log(2\\pi)$, pero no se tiene en cuenta ya que al tener una constante aditiva en todas las clases, no afecta al cálculo del máximo.\n",
        "\n",
        "### LDA\n",
        "\n",
        "En el caso de LDA se hace una suposición extra, que es $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma)$, es decir que las poblaciones no sólo siguen una distribución normal sino que son de igual matriz de covarianzas. Reemplazando arriba se obtiene entonces:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  -\\frac{1}{2}\\log |\\Sigma| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "Ahora, como $-\\frac{1}{2}\\log |\\Sigma|$ es común a todas las clases se puede incorporar a la constante aditiva y, distribuyendo y reagrupando términos sobre $(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)$ se obtiene finalmente:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
        "$$\n",
        "\n",
        "### Entrenamiento/Ajuste\n",
        "\n",
        "Obsérvese que para ambos modelos, ajustarlos a los datos implica estimar los parámetros $(\\mu_j, \\Sigma_j) \\; \\forall j = 1, \\dots, k$ en el caso de QDA, y $(\\mu_j, \\Sigma)$ para LDA.\n",
        "\n",
        "Estos parámetros se estiman por máxima verosimilitud, de manera que los estimadores resultan:\n",
        "\n",
        "* $\\hat{\\mu}_j = \\bar{x}_j$ el promedio de los $x$ de la clase *j*\n",
        "* $\\hat{\\Sigma}_j = s^2_j$ la matriz de covarianzas estimada para cada clase *j*\n",
        "* $\\hat{\\pi}_j = f_{R_j} = \\frac{n_j}{n}$ la frecuencia relativa de la clase *j* en la muestra\n",
        "* $\\hat{\\Sigma} = \\frac{1}{n} \\sum_{j=1}^k n_j \\cdot s^2_j$ el promedio ponderado (por frecs. relativas) de las matrices de covarianzas de todas las clases. *Observar que se utiliza el estimador de MV y no el insesgado*\n",
        "\n",
        "Es importante notar que si bien todos los $\\mu, \\Sigma$ deben ser estimados, la distribución *a priori* puede no inferirse de los datos sino asumirse previamente, utilizándose como entrada del modelo.\n",
        "\n",
        "### Predicción\n",
        "\n",
        "Para estos modelos, al igual que para cualquier clasificador Bayesiano del tipo antes visto, la estimación de la clase es por método *plug-in* sobre la regla de decisión $H(x)$, es decir devolver la clase que maximiza $\\hat{f}_j(x) \\cdot \\hat{\\pi}_j$, o lo que es lo mismo $\\log\\hat{f}_j(x) + \\log\\hat{\\pi}_j$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TDWOgpJWKQa"
      },
      "source": [
        "## Estructura del código"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yEV8WbiWl6k"
      },
      "source": [
        "## Modelo "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "teF9O9JJmG7Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import det, inv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sDBLvbTtlwzs"
      },
      "outputs": [],
      "source": [
        "class ClassEncoder:\n",
        "  def fit(self, y):\n",
        "    self.names = np.unique(y)\n",
        "    self.name_to_class = {name:idx for idx, name in enumerate(self.names)}\n",
        "    self.fmt = y.dtype\n",
        "    # Q1: why is there no need for class_to_name?\n",
        "\n",
        "  def _map_reshape(self, f, arr):\n",
        "    return np.array([f(elem) for elem in arr.flatten()]).reshape(arr.shape)\n",
        "    # Q2: why is reshaping necessary?\n",
        "\n",
        "  def transform(self, y):\n",
        "    return self._map_reshape(lambda name: self.name_to_class[name], y)\n",
        "\n",
        "  def fit_transform(self, y):\n",
        "    self.fit(y)\n",
        "    return self.transform(y)\n",
        "\n",
        "  def detransform(self, y_hat):\n",
        "    return self._map_reshape(lambda idx: self.names[idx], y_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m0KYC8_uSOu4"
      },
      "outputs": [],
      "source": [
        "class BaseBayesianClassifier:\n",
        "  def __init__(self):\n",
        "    self.encoder = ClassEncoder()\n",
        "\n",
        "  def _estimate_a_priori(self, y):\n",
        "    a_priori = np.bincount(y.flatten().astype(int)) / y.size\n",
        "    # Q3: what does bincount do?\n",
        "    return np.log(a_priori)\n",
        "    \n",
        "  def _fit_params(self, X, y):\n",
        "    # estimate all needed parameters for given model\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def fit(self, X, y, a_priori=None):\n",
        "    \n",
        "    # first encode the classes\n",
        "    y = self.encoder.fit_transform(y)\n",
        "\n",
        "    # if it's needed, estimate a priori probabilities\n",
        "    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n",
        "\n",
        "    # check that a_priori has the correct number of classes\n",
        "    assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n",
        "\n",
        "    # now that everything else is in place, estimate all needed parameters for given model\n",
        "    self._fit_params(X, y)\n",
        "    # Q4: why do we need to do this last, can't we do it first?\n",
        "\n",
        "  def predict(self, X):\n",
        "    # this is actually an individual prediction encased in a for-loop\n",
        "    m_obs = X.shape[1]\n",
        "    y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n",
        "    \n",
        "    for i in range(m_obs):\n",
        "      encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n",
        "      y_hat[i] = self.encoder.names[encoded_y_hat_i]\n",
        "\n",
        "    # return prediction as a row vector (matching y)\n",
        "    return y_hat.reshape(1,-1)\n",
        "\n",
        "  def _predict_one(self, x):\n",
        "    # calculate all log posteriori probabilities (actually, +C)\n",
        "    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i in enumerate(self.log_a_priori) ]\n",
        "\n",
        "    # return the class that has maximum a posteriori probability\n",
        "    return np.argmax(log_posteriori)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IRamFdiGDuSR"
      },
      "outputs": [],
      "source": [
        "class QDA(BaseBayesianClassifier):\n",
        "\n",
        "  def _fit_params(self, X, y):\n",
        "    # estimate each covariance matrix\n",
        "    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True)) for idx in range(len(self.log_a_priori))]\n",
        "    # Q5: why not just X[:,y==idx]?\n",
        "    # Q6: what does bias=True mean? why not use bias=False?\n",
        "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True) \n",
        "                  for idx in range(len(self.log_a_priori))]\n",
        "    # Q7: what does axis=1 mean? why not axis=0 instead?\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used\n",
        "    inv_cov = self.inv_covs[class_idx]\n",
        "    unbiased_x =  x - self.means[class_idx]\n",
        "    return 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QDA_Original(BaseBayesianClassifier):\n",
        "\n",
        "  def _fit_params(self, X, y):\n",
        "    # estimate each covariance matrix\n",
        "    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True)) for idx in range(len(self.log_a_priori))]\n",
        "    # Q5: why not just X[:,y==idx]?\n",
        "    # Q6: what does bias=True mean? why not use bias=False?\n",
        "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True) \n",
        "                  for idx in range(len(self.log_a_priori))]\n",
        "    # Q7: what does axis=1 mean? why not axis=0 instead?\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used\n",
        "    inv_cov = self.inv_covs[class_idx]\n",
        "    unbiased_x =  x - self.means[class_idx]\n",
        "    return -0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS_zoK-gWkRf"
      },
      "source": [
        "## Código para pruebas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "m05KrhUDINVs"
      },
      "outputs": [],
      "source": [
        "# hiperparámetros\n",
        "rng_seed = 6543"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hkXcoldXOqs",
        "outputId": "b07a5027-be83-4c0a-a09e-e4f3a21e4c5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X: (150, 4), Y:(150, 1)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "def get_iris_dataset():\n",
        "  data = load_iris()\n",
        "  X_full = data.data\n",
        "  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
        "  return X_full, y_full\n",
        "\n",
        "X_full, y_full = get_iris_dataset()\n",
        "\n",
        "print(f\"X: {X_full.shape}, Y:{y_full.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAk-UQCjKecT",
        "outputId": "ddf4e2f6-1baf-4a51-de72-5ce1d7c03db8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# peek data matrix\n",
        "X_full[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdzMURX2KVdO",
        "outputId": "66a3cd6b-7dda-4618-b13f-628d113bf7d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([['setosa'],\n",
              "       ['setosa'],\n",
              "       ['setosa'],\n",
              "       ['setosa'],\n",
              "       ['setosa']], dtype='<U10')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# peek target vector\n",
        "y_full[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.unique(y_full)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKP_QmWCIECs",
        "outputId": "36c28bcc-5d33-43e6-f231-3f3bf7b460cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 90) (1, 90) (4, 60) (1, 60)\n"
          ]
        }
      ],
      "source": [
        "# preparing data, train - test validation\n",
        "# 70-30 split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.4, random_state=rng_seed)\n",
        "\n",
        "# traspose everything because this model format assumes column vectors\n",
        "train_x = X_train.T\n",
        "train_y = y_train.T\n",
        "test_x = X_test.T\n",
        "test_y = y_test.T\n",
        "\n",
        "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dGIf2TA5SpoT"
      },
      "outputs": [],
      "source": [
        "qda = QDA()\n",
        "\n",
        "qda.fit(train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0Q30DyLWpTL",
        "outputId": "c113c448-5230-44be-8f85-7a6d3f732d8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train (apparent) error is 0.0111 while test error is 0.0167\n"
          ]
        }
      ],
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "  return (y_true == y_pred).mean()\n",
        "\n",
        "train_acc = accuracy(train_y, qda.predict(train_x))\n",
        "test_acc = accuracy(test_y, qda.predict(test_x))\n",
        "print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1Yb1V7_yXRfO"
      },
      "source": [
        "# Consigna\n",
        "\n",
        "## Implementación\n",
        "1. Entrenar un modelo QDA utilizando ahora una *a priori* uniforme ¿Se observan diferencias?¿Por qué?\n",
        "2. Implementar el modelo LDA, entrenarlo y testearlo contra los mismos sets que QDA ¿Se observan diferencias? ¿Podría decirse que alguno de los dos es notoriamente mejor que el otro?\n",
        "3. Utilizar otros 2 (dos) valores de *random seed* para obtener distintos splits de train y test, y repetir la comparación del punto anterior ¿Qué se observa?\n",
        "1. *(Opcional)* En `BaseBayesianClassifier._predict_one` se estima la posteriori de cada clase por separado, a pesar de que la cuenta es siempre la misma (cambiando valores de parámetros, pero no dimensiones). Aprovechando el *broadcasting* de NumPy, se puede reemplazar ese list comprehension por un cálculo *tensorizado* donde tanto medias como varianzas (o inversas) estén \"stackeadas\" sobre un tercer eje, permitiendo el cálculo en paralelo de todas las clases con un:\n",
        "`log_posteriori = self.tensor_log_a_priori + self._predict_log_conditionals(x)`. Implementar dicha modificación y mostrar su uso. *Ayuda: los métodos `np.stack` y la documentación del operador [`@`](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html) son de gran utilidad.*\n",
        "\n",
        "## Preguntas técnicas\n",
        "\n",
        "Responder las 7 preguntas que se encuentran distribuidas a lo largo del código.\n",
        "\n",
        "## Preguntas teóricas\n",
        "\n",
        "1. En LDA se menciona que la función a maximizar puede ser, mediante operaciones, convertida en:\n",
        "$$\n",
        "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
        "$$\n",
        "Mostrar los pasos por los cuales se llega a dicha expresión.\n",
        "\n",
        "2. Explicar, utilizando las respectivas funciones a maximizar, por qué QDA y LDA son \"quadratic\" y \"linear\".\n",
        "\n",
        "3. La implementación de QDA estima la probabilidad condicional utilizando `0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x` que no es *exactamente* lo descrito en el apartado teórico ¿Cuáles son las diferencias y por qué son expresiones equivalentes?\n",
        "\n",
        "El espíritu de esta componente práctica es la de establecer un mínimo de trabajo aceptable para su entrega; se invita al alumno a explorar otros aspectos que generen curiosidad, sin sentirse de ninguna manera limitado por la consigna.\n",
        "\n",
        "## Ejercicio teórico\n",
        "\n",
        "Sea una red neuronal de dos capas, la primera de 3 neuronas y la segunda de 1 con los parámetros inicializados con los siguientes valores:\n",
        "$$\n",
        "w^{(1)} = \n",
        "\\begin{pmatrix}\n",
        "0.1 & -0.5 \\\\\n",
        "-0.3 & -0.9 \\\\ \n",
        "0.8 & 0.02\n",
        "\\end{pmatrix},\n",
        "b^{(1)} = \\begin{pmatrix}\n",
        "0.1 \\\\\n",
        "0.5 \\\\\n",
        "0.8 \n",
        "\\end{pmatrix},\n",
        "w^{(2)} = \n",
        "\\begin{pmatrix}\n",
        "-0.4 & 0.2 & -0.5\n",
        "\\end{pmatrix},\n",
        "b^{(2)} = 0.7\n",
        "$$\n",
        "\n",
        "y donde cada capa calcula su salida vía\n",
        "\n",
        "$$\n",
        "y^{(i)} = \\sigma (w^{(i)} \\cdot x^{(i)}+b^{(i)})\n",
        "$$\n",
        "\n",
        "donde $\\sigma (z) = \\frac{1}{1+e^{-z}}$ es la función sigmoidea .\n",
        "\n",
        "\\\\\n",
        "Dada la observación $x=\\begin{pmatrix}\n",
        "1.8 \\\\\n",
        "-3.4\n",
        "\\end{pmatrix}$, $y=5$ y la función de costo $J(\\theta)=\\frac{1}{2}(\\hat{y}_\\theta-y)^2$, calcular las derivadas de J respecto de cada parámetro $w^{(1)}$, $w^{(2)}$, $b^{(1)}$, $b^{(2)}$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Solucion"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejercicio 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train (apparent) error is 0.0222 while test error is 0.0167\n"
          ]
        }
      ],
      "source": [
        "qda1 = QDA()\n",
        "\n",
        "class_count = len(np.unique(y_train))\n",
        "\n",
        "qda1.fit(train_x, train_y, a_priori=[1/class_count]*class_count)\n",
        "train_acc = accuracy(train_y, qda1.predict(train_x))\n",
        "test_acc = accuracy(test_y, qda1.predict(test_x))\n",
        "print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se observan diferencias en el entrenamiento pero no en el test. Si se observan los datos, puntualmente las y, estos no siguen una distribucion uniforme pero se aproximan mucho: las tres clases aparecen un numero similar de veces en el dataset. Si no se asigna una a priori se estimara con la muestra. Entonces usar una uniforme estricta de a priori no es tan diferente a usar la distribucion que se usa de forma automatica. Esto probablemente implica que los dos modelos finalmente son muy parecidos y eso es lo que genera la diferencia en el entrenamiento y obtener el mismo valor en el test. De todas formas con una sola prueba es dificil determinar estrictamente que es lo que esta pasando."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.3       , 0.32222222, 0.37777778])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ce = ClassEncoder()\n",
        "\n",
        "np.bincount(ce.fit_transform(y_train).flatten().astype(int)) / y_train.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QDA INV COVS: \n",
            "[array([[ 22.09817859, -15.69123833,  -3.54800691,   0.58063186],\n",
            "       [-15.69123833,  19.57946289,   1.25130614,  -9.72088256],\n",
            "       [ -3.54800691,   1.25130614,  55.34972828,  -9.87952557],\n",
            "       [  0.58063186,  -9.72088256,  -9.87952557, 106.5198752 ]]), array([[ 10.9780256 ,  -5.88010888, -10.9906851 ,  10.30978873],\n",
            "       [ -5.88010888,  21.0508669 ,   5.57540582, -23.9370431 ],\n",
            "       [-10.9906851 ,   5.57540582,  31.49015035, -54.61305166],\n",
            "       [ 10.30978873, -23.9370431 , -54.61305166, 151.02211857]]), array([[ 10.26219903,  -6.09026899,  -9.10771516,   3.33185775],\n",
            "       [ -6.09026899,  22.47444984,   0.9369471 , -12.68803238],\n",
            "       [ -9.10771516,   0.9369471 ,  13.89585808,  -2.5501201 ],\n",
            "       [  3.33185775, -12.68803238,  -2.5501201 ,  19.38626185]])]\n",
            "QDA LOG A PRIORI: [-1.2039728  -1.13251384 -0.97344915]\n",
            "QDA UNIFORM INV COVS: \n",
            "[array([[ 22.09817859, -15.69123833,  -3.54800691,   0.58063186],\n",
            "       [-15.69123833,  19.57946289,   1.25130614,  -9.72088256],\n",
            "       [ -3.54800691,   1.25130614,  55.34972828,  -9.87952557],\n",
            "       [  0.58063186,  -9.72088256,  -9.87952557, 106.5198752 ]]), array([[ 10.9780256 ,  -5.88010888, -10.9906851 ,  10.30978873],\n",
            "       [ -5.88010888,  21.0508669 ,   5.57540582, -23.9370431 ],\n",
            "       [-10.9906851 ,   5.57540582,  31.49015035, -54.61305166],\n",
            "       [ 10.30978873, -23.9370431 , -54.61305166, 151.02211857]]), array([[ 10.26219903,  -6.09026899,  -9.10771516,   3.33185775],\n",
            "       [ -6.09026899,  22.47444984,   0.9369471 , -12.68803238],\n",
            "       [ -9.10771516,   0.9369471 ,  13.89585808,  -2.5501201 ],\n",
            "       [  3.33185775, -12.68803238,  -2.5501201 ,  19.38626185]])]\n",
            "QDA UNIFORM LOG A PRIORI: [-1.09861229 -1.09861229 -1.09861229]\n"
          ]
        }
      ],
      "source": [
        "print(f\"QDA INV COVS: \\n{qda.inv_covs}\")\n",
        "print(f\"QDA LOG A PRIORI: {qda.log_a_priori}\")\n",
        "\n",
        "print(f\"QDA UNIFORM INV COVS: \\n{qda1.inv_covs}\")\n",
        "print(f\"QDA UNIFORM LOG A PRIORI: {qda1.log_a_priori}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejercicio 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LDA(BaseBayesianClassifier):\n",
        "    \n",
        "    def _fit_params(self, X, y):\n",
        "      # estimate each covariance matrix\n",
        "      self.inv_covs = [inv(np.cov(X[:,y.flatten()==y.flatten()[0]], bias=True))]\n",
        "      # Q5: why not just X[:,y==idx]?\n",
        "      # Q6: what does bias=True mean? why not use bias=False?\n",
        "      self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True) \n",
        "                    for idx in range(len(self.log_a_priori))]\n",
        "      # Q7: what does axis=1 mean? why not axis=0 instead?\n",
        "\n",
        "    def _predict_log_conditional(self, x, class_idx):\n",
        "      # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "      # this should depend on the model used\n",
        "      inv_cov = self.inv_covs\n",
        "      return self.means[class_idx].T @ inv_cov @ (x-0.5*self.means[class_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train (apparent) error is 0.0333 while test error is 0.0167\n"
          ]
        }
      ],
      "source": [
        "lda = LDA()\n",
        "\n",
        "lda.fit(train_x, train_y)\n",
        "\n",
        "train_acc = accuracy(train_y, lda.predict(train_x))\n",
        "test_acc = accuracy(test_y, lda.predict(test_x))\n",
        "print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aunque en test son identicos en entrenamiento QDA es 3 veces mejor que LDA. Dado que en test performan de la misma manera no hay razon para decir que uno es necesariamente mejor que el otro, aunque QDA logra ajustarse a los datos con los que entrena mejor que LDA"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejercicio 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 90) (1, 90) (4, 60) (1, 60)\n"
          ]
        }
      ],
      "source": [
        "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_full, y_full, test_size=0.4, random_state=42)\n",
        "\n",
        "# traspose everything because this model format assumes column vectors\n",
        "train_x_1 = X_train_1.T\n",
        "train_y_1 = y_train_1.T\n",
        "test_x_1 = X_test_1.T\n",
        "test_y_1 = y_test_1.T\n",
        "\n",
        "print(train_x_1.shape, train_y_1.shape, test_x_1.shape, test_y_1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train (apparent) error is 0.0444 while test error is 0.0000\n"
          ]
        }
      ],
      "source": [
        "qda.fit(train_x_1, train_y_1)\n",
        "\n",
        "train_acc = accuracy(train_y_1, lda.predict(train_x_1))\n",
        "test_acc = accuracy(test_y_1, lda.predict(test_x_1))\n",
        "print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train (apparent) error is 0.0222 while test error is 0.0167\n"
          ]
        }
      ],
      "source": [
        "lda.fit(train_x_1, train_y_1)\n",
        "\n",
        "train_acc = accuracy(train_y_1, lda.predict(train_x_1))\n",
        "test_acc = accuracy(test_y_1, lda.predict(test_x_1))\n",
        "print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para este split QDA fue perfecto en test, por lo que se lo podria considerar mejor en este escenario, aunque no se logro adaptar al train set de la misma manera que lo logro LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 90) (1, 90) (4, 60) (1, 60)\n"
          ]
        }
      ],
      "source": [
        "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_full, y_full, test_size=0.4, random_state=36)\n",
        "\n",
        "# traspose everything because this model format assumes column vectors\n",
        "train_x_2 = X_train_2.T\n",
        "train_y_2 = y_train_2.T\n",
        "test_x_2 = X_test_2.T\n",
        "test_y_2 = y_test_2.T\n",
        "\n",
        "print(train_x_2.shape, train_y_2.shape, test_x_2.shape, test_y_2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train (apparent) error is 0.0333 while test error is 0.0000\n"
          ]
        }
      ],
      "source": [
        "qda.fit(train_x_2, train_y_2)\n",
        "\n",
        "train_acc = accuracy(train_y_2, lda.predict(train_x_2))\n",
        "test_acc = accuracy(test_y_2, lda.predict(test_x_2))\n",
        "print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train (apparent) error is 0.0222 while test error is 0.0000\n"
          ]
        }
      ],
      "source": [
        "lda.fit(train_x_2, train_y_2)\n",
        "\n",
        "train_acc = accuracy(train_y_2, lda.predict(train_x_2))\n",
        "test_acc = accuracy(test_y_2, lda.predict(test_x_2))\n",
        "print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este caso los dos pudieron predecir de forma perfecta en test. QDA se ajusto menos a los valores de entrenamiento. Observando estos 3 experimentos es dificil determinar si alguno de los dos es objetivamente mejor, aunque QDA parece ser un poco mas confiable, dado que en las pruebas de test fue un poco mejor"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Respuestas\n",
        "\n",
        "## Preguntas tecnicas\n",
        "\n",
        "1) Porque se puede encontrar en self.names. Si se pide self.names[x] el resultado sera concordante a lo que esta en name_to_class\n",
        "\n",
        "2) Porque se descompuso la matriz en un arreglo para poder aplicar facilmente la funcion pero es necesario volver a armar una matriz de las mismas dimensiones y con esos valores calculados y no devolver el arreglo que se uso para calcular\n",
        "\n",
        "3) bincount cuenta la frecuencia de todos los numeros mayores o iguales a 0 dentro de un arreglo. Lo devuelve como un arreglo donde la posicion corresponde al numero al que se le esta calculando la frecuencia. Por ejemplo, si 5 aparece 3 veces en el arreglo a np.bincount(a)[5] == 3\n",
        "\n",
        "4) No se puede porque el metodo depende de self.log_a_priori. Mientras esa variable no este asignada o ese metodo no deje de depender de esa variable no se puede sacar.\n",
        "\n",
        "5) Porque Y es una matriz y no un arreglo. Con el flatten se convierte en un arreglo y a partir de eso se puede usarlo para seleccionar los indices de X que se necesitan\n",
        "\n",
        "6) El parametro bias cambia la forma en la que se calcula la varianza muestral. Si se usa True entonces se calcula usando N y se deberia usar cuando se tiene una muestra chica. Con False se usa (N-1) y se deberia usar cuando la muestra es grande . En este caso usar True sirve, porque se es logico asumir que la muestra con la que se va a entrenar deberia ser relativamente grande\n",
        "\n",
        "7) axis sirve para determinar como aplicar la transformacion que se esta usando: si se una axis = 0 la funcion se aplica a lo largo de las columnas y si se usa axis = 1 la funcion se aplica a lo largo de las filas. En este caso el resultado es la media de cada una de las features para una clase particular. \n",
        "\n",
        "## Preguntas teoricas\n",
        "\n",
        "1 - Se parte de la funcion \n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  -\\frac{1}{2}\\log |\\Sigma| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
        "$$ \n",
        "\n",
        "$$\n",
        "C' = C -\\frac{1}{2}\\log |\\Sigma| \\Rightarrow \\log{f_j(x)} =  - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "$$\n",
        "log{f_j(x)} =  - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "$$\n",
        "log{f_j(x)} =  - \\frac{1}{2} (x^T-\\mu_j^T) \\Sigma^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "$$\n",
        "log{f_j(x)} =  - \\frac{1}{2} (x^T\\Sigma^{-1}-\\mu_j^T\\Sigma^{-1})  (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "$$\n",
        "log{f_j(x)} =  - \\frac{1}{2} (x^T\\Sigma^{-1}x - x^T\\Sigma^{-1}\\mu_j-\\mu_j^T\\Sigma^{-1}x + \\mu_j^T\\Sigma^{-1}\\mu_j) + C\n",
        "$$\n",
        "\n",
        "Como $\\Sigma^{-1}$ es Hermitiana (o simetrica si se considera que es real) entonces :\n",
        "\n",
        "$$\n",
        " x^T\\Sigma^{-1}\\mu_j = \\mu_j^T\\Sigma^{-1}x \n",
        "$$\n",
        "\n",
        "Entonces:\n",
        "\n",
        "$$\n",
        "log{f_j(x)} =  - \\frac{1}{2} (x^T\\Sigma^{-1}x - 2x^T\\Sigma^{-1}\\mu_j + \\mu_j^T\\Sigma^{-1}\\mu_j) + C\n",
        "$$\n",
        "\n",
        "$$\n",
        "log{f_j(x)} =  - \\frac{1}{2}x^T\\Sigma^{-1}x - x^T\\Sigma^{-1}\\mu_j + - \\frac{1}{2}\\mu_j^T\\Sigma^{-1}\\mu_j + C\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) - \\frac{1}{2}x^T\\Sigma^{-1}x +  C'\n",
        "$$\n",
        "\n",
        "\n",
        "Ahora es importante notar que lo relevante de cada $f_{j}(x)$ no es su valor nominal sino su valor relativo con los otros $f_{j}(x)$. \n",
        "\n",
        "Supongase que como en este caso se tienen tres clases (1, 2, 3) y una observacion $x_{1}$. Entonces se calcularian $log f_{1}(x_{1})$, $log f_{2}(x_{1})$ y $log f_{3}(x_{1})$ y se buscaria el maximo de los tres.\n",
        "\n",
        "Si se observa la ultima expresion de $\\log{f_j(x)}$ se puede observar que hay un termino que depende exclusivamente de x. Eso implica que ese termino para $log f_{1}(x_{1})$, $log f_{2}(x_{1})$ y $log f_{3}(x_{1})$\n",
        "valdra lo mismo. Y como vale lo mismo para los tres no afecta a cual de los tres es finalmente el maximo. Por ende se puede descartar sin afectar el resultado final.\n",
        "\n",
        "Entonces \n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) +  C'\n",
        "$$\n",
        "\n",
        "\n",
        "2 - Se puede ver que en el QDA hay un producto de $x^T$ con x, lo que hace que aparezca $x^2$, en el LDA solo aparece x, por lo tanto en uno es lineal y en el otro es cuadrático con x. \n",
        "\n",
        "![alt text for screen readers](images/qda-lda.jpeg \"Text to show on mouseover\")\n",
        "\n",
        "\n",
        "3 -  \n",
        "Formula implementada:  \n",
        "\n",
        "$$\\frac{1}{2} log|\\Sigma _j|^{-1}-\\frac{1}{2}(x-\\mu _j)^T \\Sigma _j ^{-1}(x-\\mu _j)$$\n",
        "\n",
        "Formula teórica: \n",
        "\n",
        "$$-\\frac{1}{2} log|\\Sigma _j|-\\frac{1}{2}(x-\\mu _j)^T \\Sigma _j ^{-1}(x-\\mu _j)+C$$\n",
        "\n",
        "Se debe demostrar que: \n",
        "\n",
        "$$\\frac{1}{2} log|\\Sigma _j|^{-1}-\\frac{1}{2}(x-\\mu _j)^T \\Sigma _j ^{-1}(x-\\mu _j)=-\\frac{1}{2} log|\\Sigma _j|-\\frac{1}{2}(x-\\mu _j)^T \\Sigma _j ^{-1}(x-\\mu _j)+C$$\n",
        "\n",
        "Sumando en ambos lados $\\frac{1}{2}(x-\\mu _j)^T \\Sigma _j ^{-1}(x-\\mu _j)$: \n",
        "\n",
        "$$\\frac{1}{2} log|\\Sigma _j|^{-1}=-\\frac{1}{2} log|\\Sigma _j|+C$$\n",
        "\n",
        "\n",
        "Por propiedad de logaritmo, $log|\\Sigma _j|^{-1}=-log|\\Sigma _j|$, entonces:\n",
        "\n",
        "$$0=C$$ \n",
        "\n",
        "Por lo tanto se demuestra que la formula implementada es la misma que la formula teórica utilizando C=0."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejercicio teorico"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para este ejercicio se tiene el siguiente grafo que representa las diferentes capas de la red neuronal\n",
        "\n",
        "![alt text for screen readers](images/grafo2.png \"Text to show on mouseover\")\n",
        "\n",
        "El primer paso es calcular $Z1$, $Y1$, $Z2$, $Ypred$ y $J$.\n",
        "\n",
        "$$\n",
        "\n",
        "Z1 = \\begin{pmatrix}\n",
        "1.98 \\\\\n",
        "3.02 \\\\ \n",
        "2.172 \n",
        "\\end{pmatrix} ,\n",
        "\n",
        "Y1 = \\begin{pmatrix}\n",
        "0.8786 \\\\\n",
        "0.9534 \\\\ \n",
        "0.8977 \n",
        "\\end{pmatrix} ,\n",
        "\n",
        "Z2 = 0.0903 ,\n",
        "\n",
        "Ypred = 0.5225 \n",
        "\n",
        "$$\n",
        "\n",
        "Se calcula tambien la derivada de $\\sigma$\n",
        "$$\n",
        "\\sigma'(x) = \\frac{-e^x}{2 e^x + (e^x)^2 + 1}\n",
        "$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora es necesario definir las derivadas que se buscan calcular. Estas se pueden definir de la siguiente manera usando la regla de la cadena:\n",
        "\n",
        "$$\n",
        "\n",
        "\\frac{dJ}{dw_{2}} = \\frac{dJ}{dY_{pred}}  \\frac{dY_{pred}}{dz_{2}} \\frac{dz_{2}}{dw_{2}}\n",
        "\n",
        "$$\n",
        "\n",
        "$$\n",
        "\n",
        "\\frac{dJ}{db_{2}} = \\frac{dJ}{dY_{pred}}  \\frac{dY_{pred}}{dz_{2}} \\frac{dz_{2}}{db_{2}} \n",
        "\n",
        "$$\n",
        "\n",
        "$$\n",
        "\n",
        "\\frac{dJ}{dw_{1}} = \\frac{dJ}{dY_{pred}}  \\frac{dY_{pred}}{dz_{2}} \\frac{dz_{2}}{dY1} \\frac{dY1}{dz_{1}} \\frac{dz_{1}}{dw_{1}}\n",
        "\n",
        "$$\n",
        "\n",
        "$$\n",
        "\n",
        "\\frac{dJ}{db_{1}} = \\frac{dJ}{dY_{pred}}  \\frac{dY_{pred}}{dz_{2}} \\frac{dz_{2}}{dY1} \\frac{dY1}{dz_{1}} \\frac{dz_{1}}{db_{1}}\n",
        "\n",
        "$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para $\\frac{dJ}{dw_{2}}$\n",
        "\n",
        "$$\n",
        "\\frac{dJ}{dY_{pred}} = (Y_{pred} - Y) ,\n",
        "\n",
        "\\frac{dY_{pred}}{dz_{2}} = \\sigma'(z2),\n",
        "\n",
        "\\frac{dz_{2}}{dw_{2}} = Y1\n",
        "\n",
        "$$\n",
        "\n",
        "$$\n",
        "\n",
        "\\frac{dJ}{dw_{2}} = \\frac{dJ}{dY_{pred}}  \\frac{dY_{pred}}{dz_{2}} \\frac{dz_{2}}{dw_{2}}\n",
        "\n",
        "\\Rightarrow \n",
        "\n",
        "\\frac{dJ}{dw_{2}} = (Y_{pred} - Y) \\sigma'(z2) Y1^T\n",
        "\n",
        "$$\n",
        "\n",
        "\n",
        "Reemplazando con los valores conocidos\n",
        "\n",
        "$$\n",
        "\n",
        "\\frac{dJ}{dw_{2}} = (0.5225 - 5) \\sigma'(0.0903) \\begin{pmatrix}\n",
        "0.8786 \\\\\n",
        "0.9534 \\\\ \n",
        "0.8977 \n",
        "\\end{pmatrix} ^T = \\begin{pmatrix}\n",
        "-0.98155 \\\\\n",
        "-1.06509 \\\\ \n",
        "-1.00280 \n",
        "\\end{pmatrix}^T\n",
        "\n",
        "$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para $\\frac{dJ}{db_{2}}$\n",
        "\n",
        "$$\n",
        "\n",
        "\\frac{dz_{2}}{db_{2}} = 1\n",
        "\n",
        "$$\n",
        "\n",
        "$$\n",
        "\n",
        "\\frac{dJ}{db_{2}} = \\frac{dJ}{dY_{pred}}  \\frac{dY_{pred}}{dz_{2}} \\frac{dz_{2}}{db_{2}}\n",
        "\n",
        "\\Rightarrow \n",
        "\n",
        "\\frac{dJ}{db_{2}} = (Y_{pred} - Y) \\sigma'(z2) 1\n",
        "\n",
        "$$\n",
        "\n",
        "Reemplazando con los valores conocidos\n",
        "\n",
        "$$\n",
        "\n",
        "\\frac{dJ}{db_{2}} = (0.5225 - 5) \\sigma'(0.0903) = -1.11707\n",
        "\n",
        "$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para $\\frac{dJ}{dw_{1}}$\n",
        "\n",
        "$$\n",
        "\n",
        "\\frac{dY_{pred}}{dz_{2}} = \\sigma'(z2),\n",
        "\n",
        "\\frac{dz_{2}}{dY1} = w_{2},\n",
        "\n",
        "\\frac{dY1}{dz_{1}} = \\sigma'(z1),\n",
        "\n",
        "\\frac{dz_{1}}{dw_{1}} = X1\n",
        "\n",
        "$$\n",
        "\n",
        "$$\n",
        "\n",
        "\\frac{dJ}{dw_{1}} = \\frac{dJ}{dY_{pred}}  \\frac{dY_{pred}}{dz_{2}} \\frac{dz_{2}}{dY1} \\frac{dY1}{dz_{1}} \\frac{dz_{1}}{dw_{1}}\n",
        "\n",
        "\\Rightarrow \n",
        "\n",
        "\\frac{dJ}{dw_{1}} = (Y_{pred} - Y)  \\sigma'(z2) w_{2}^T  \\sigma'(z1) X1^T\n",
        "\n",
        "$$\n",
        "\n",
        "Reemplazando con los valores conocidos\n",
        "\n",
        "$$\n",
        "\n",
        "\\frac{dJ}{dw_{1}} = (0.5225 - 5) \\sigma'(0.0903) \\begin{pmatrix}\n",
        "-0.4 & 0.2 & -0.5\n",
        "\\end{pmatrix}^T \\odot \\sigma'(\\begin{pmatrix}\n",
        "1.98 \\\\\n",
        "3.02 \\\\ \n",
        "2.172 \n",
        "\\end{pmatrix}) \\begin{pmatrix}\n",
        "1.8 \\\\\n",
        "-3.4\n",
        "\\end{pmatrix} ^T = \\begin{pmatrix}\n",
        "0.0857 & -0.1619 \\\\\n",
        "- 0.0178 & 0.033 \\\\\n",
        "0.0923 & -0.1743 \n",
        "\\end{pmatrix}\n",
        "\n",
        "$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para $\\frac{dJ}{db_{1}}$\n",
        "\n",
        "$$\n",
        "\n",
        "\\frac{dz_{1}}{db_{1}} = 1\n",
        "\n",
        "$$\n",
        "\n",
        "$$\n",
        "\n",
        "\\frac{dJ}{db_{1}} = \\frac{dJ}{dY_{pred}}  \\frac{dY_{pred}}{dz_{2}} \\frac{dz_{2}}{dY1} \\frac{dY1}{dz_{1}} \\frac{dz_{1}}{db_{1}}\n",
        "$$\n",
        "\n",
        "Reemplazando con los valores conocidos\n",
        "\n",
        "$$\n",
        "\n",
        "\\frac{dJ}{db_{1}} = ((0.5225 - 5) \\sigma'(0.0903) \\begin{pmatrix}\n",
        "-0.4 & 0.2 & -0.5\n",
        "\\end{pmatrix}^T \\odot \\sigma'(\\begin{pmatrix}\n",
        "1.98 \\\\\n",
        "3.02 \\\\ \n",
        "2.172 \n",
        "\\end{pmatrix}) =\\begin{pmatrix}\n",
        "0.0476 \\\\\n",
        "-0.0099 \\\\ \n",
        "0.0512 \n",
        "\\end{pmatrix}\n",
        "\n",
        "$$"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
